# üõ°Ô∏è **Detec√ß√£o de Varredura de Portas (PortScan) com Machine Learning**

Este reposit√≥rio cont√©m o pipeline completo de aprendizado de m√°quina desenvolvido para a detec√ß√£o de ataques de PortScan utilizando telemetria de rede baseada em fluxos. O projeto faz parte do Trabalho de Conclus√£o de Curso (TCC) em Redes de Computadores no IFPB.




## üìã Resumo do Projeto

O objetivo principal √© identificar precocemente atividades de reconhecimento (varreduras de portas) em redes corporativas, utilizando uma abordagem reprodut√≠vel que lida com o alto desbalanceamento de dados t√≠pico deste cen√°rio.

* **Dataset**: CICIDS2017 (vers√£o pr√©-processada).
* **Modelo de Melhor Desempenho**: Random Forest + SMOTE.
* **Resultados Finais**: 99,49% de Recall e apenas 0,05% de taxa de falsos positivos (FPR).




## üöÄ Estrutura do Pipeline

O pipeline foi desenvolvido de forma modular para garantir transpar√™ncia e reprodutibilidade:

**1 - Prepara√ß√£o do Ambiente**: Instala√ß√£o e importa√ß√£o das bibliotecas (Pandas, Scikit-learn, Imbalanced-learn, XGBoost, LightGBM).

**2 - Pr√©-processamento**: Tratamento de valores ausentes (imputa√ß√£o pela mediana) e padroniza√ß√£o de escalas.

**3 - Sele√ß√£o de Atributos**: Uso do teste estat√≠stico F para selecionar as 30 caracter√≠sticas mais relevantes.

**4 - Balanceamento**: Aplica√ß√£o da t√©cnica SMOTE para equilibrar a classe minorit√°ria (ataques) no conjunto de treino.

**5 - Treinamento e Valida√ß√£o**: Compara√ß√£o entre Regress√£o Log√≠stica (Baseline), Random Forest, XGBoost e LightGBM via Valida√ß√£o Cruzada Estratificada (5-fold).




## üìä Principais Resultados

Os modelos foram avaliados em um conjunto de dados "n√£o visto" (held-out) para simular o comportamento em um ambiente real:
| Modelo | Precision | Recall | F1-score | ROC-AUC | FPR |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **Random Forest + SMOTE** | **0,8801** | **0,9949** | **0,9340** | **0,9987** | **0,05%** |
| LightGBM + SMOTE | 0,8647 | 0,9974 | 0,9264 | 0,9980 | 0,05% |
| XGBoost + SMOTE | 0,8442 | 0,9974 | 0,9144 | 0,9992 | 0,06% |
| Logistic Regression (baseline) | 0,1992 | 0,9693 | 0,3304 | 0,9967 | 1,32% |




## üõ†Ô∏è Como Reproduzir

**1 -** Acesse o notebook atrav√©s do arquivo .ipynb neste reposit√≥rio.

**2 -** Carregue o dataset CICIDS2017 conforme as instru√ß√µes contidas no notebook.

**3 -** Execute as c√©lulas sequencialmente para observar a gera√ß√£o do ranking de features e o treinamento dos modelos.




## üéì Autor

### J√∫lio C√©zar Netto de Ara√∫jo 

**Orientador**: Prof. Diego Ernesto Rosa Pessoa.

**Institui√ß√£o**: Instituto Federal da Para√≠ba (IFPB) - Unidade Acad√™mica de Informa√ß√£o e Comunica√ß√£o.
